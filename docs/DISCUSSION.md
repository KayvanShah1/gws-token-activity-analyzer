# ğŸ§  Project Decision Log & Architecture Reflections

This document captures the thought process, alternatives considered, and reasons for choosing specific tools and architecture for the gws-token-activity-analyzer project. It's not just a READMEâ€”it's a **tech journal** of how we designed a production-leaning pipeline with real-world constraints in mind. âœ¨

## ğŸ› ï¸ Overview of Pipeline Design

text
Google Admin Reports API
        â†“
 Buffered API Fetcher with Retry & Overlap Logic
        â†“
 Partitioned Raw Files (JSONL, per hour)
        â†“
 Beam-based Transformer Pipeline
        â†“
 Partitioned Parquet Files (per day)
        â†“
 Polars-based Ad Hoc Analytics


## 1. ğŸ¯ Data Ingestion Design

### âœ… Chosen Approach:

* Use **Authorized Google API session** with retry strategy (exponential backoff).
* Fetch activity logs since last run using a **state checkpoint file**.
* Add **3-minute overlap** to prevent event loss.
* Store raw events as **JSONL per hour**, using buffered writes.

### ğŸ’­ Ideas Explored:

* Saving one file per request was too noisy â€” switched to partitioned writes with a buffer.
* Also created **per-run JSONL logs** (gzip) for auditing/debugging.

### ğŸ’¡ Takeaways:

* Adding a time overlap + deduplication downstream = safer against late or re-ordered events.
* Per-run logs can serve as a "recovery checkpoint" or forensic trail if anything breaks.

## 2. âš™ï¸ Data Transformation & Processing

### âœ… Chosen Tool: **Apache Beam** (with DirectRunner)

### Why Beam?

* **Scalable, parallel processing** with easy deduplication via CombinePerKey.
* Cleaner for grouped outputs (e.g., daily Parquet).
* Each run processes 5â€“6 files with 5kâ€“20k records each, totaling \~500K records.

### ğŸ§ª Ideas Tried:

* Considered **Polars** or **Pandas** for transformation.

  * âŒ Inefficient for transforming millions of rows with dedup + grouping logic.
  * âŒ Not ideal for parallel writes to partitioned Parquet.
* Beam was **slower** (\~15 mins) than Polars, but gave us better structure and extensibility.

### ğŸ’¡ Nuance:

* For large-scale daily processing with flexible partitioning: Beam > Polars.
* For ad-hoc slicing/dicing: Polars wins.

## 3. ğŸ’¾ Data Storage Strategy

* Raw Events: JSONL per hour (data/raw/YYYY-MM-DD/part_HH.jsonl.gz)
* Transformed Events: Parquet per day (data/processed/events_YYYY-MM-DD.parquet)
* Per-run Audit: Gzipped JSONL (data/per_run/epr_<timestamp>.jsonl.gz)
* State File: JSON file storing last-run timestamp (state/fetcher_state.json)

### ğŸ§  Tradeoffs Considered:

* Could have used a DB (e.g., SQLite, TimescaleDB):

  * âŒ Added infra, queries, schema constraints.
  * âŒ Not ideal for large-scale I/O-based transformation.
* Decided on **filesystem-based datalake** layout, good enough for 2-dayly batch pipeline.

## 4. ğŸ“Š Analytics Layer

### âœ… Tool: **Polars**

* Ultra-fast DataFrame queries
* Easily filters & groups large Parquet datasets
* Ideal for batch analytics like:

  * Top users
  * API method with highest bytes
  * Hourly/daily event trends

### Explored Alternatives:

* **DuckDB**:

  * ğŸŸ¡ Faster for SQL-style queries & joins
  * âŒ No native JSON support (yet), so less useful at ingest stage
  * âœ… Excellent complement to Polars for exploratory or dashboard-style querying

## 5. ğŸ§± Observability & Logging

* Used **Pythonâ€™s logging module** with RotatingFileHandler.
* Integrated **Rich** for local console logging with timestamps.
* Logging includes:

  * File writes
  * Run duration
  * Per-run file locations
  * Retry/overlap status

## 6. ğŸ§ª Testing and Safety Measures

* Added buffer flushing with sizes (e.g., 5K per partition, 2.5K per-run).
* Manual testing showed no data loss or duplicates after transformation.
* Logs rotate and persist in /logs.

## 7. ğŸš€ Future Considerations

| Feature                | Why Consider It?                       |
| ---------------------- | -------------------------------------- |
| main.py orchestrator | Wrap fetch + process + analyze cleanly |
| DuckDB for analytics | Interactive SQL-like analysis layer    |
| API pagination counter | Estimate future load from page tokens  |
| DB support (opt-in)    | For continuous/live streaming systems  |
| Data schema registry   | Track versioning of event formats      |

## ğŸ¤¹ Summary of Tools

| Task             | Tool Used                 | Reason                            |
| ---------------- | ------------------------- | --------------------------------- |
| Fetch Events     | requests + retry + auth | Resilient API access              |
| Transform/Dedup  | Apache Beam             | Parallel, grouped transformations |
| Save as Parquet  | PyArrow                 | Fast columnar storage             |
| Ad-hoc Analytics | Polars                  | Blazing-fast queries              |
| Buffering        | Lists + flush           | Avoids disk I/O flood             |
| Logging & State  | logging + JSON          | Traceability + Resumability       |

Absolutely, letâ€™s enrich the summary with those key offshoot discussions and design explorations you made while evaluating different approaches.

---

## ğŸ”„ Data Processing Approaches Explored

We explored **multiple data processing frameworks** before settling on Apache Beam. Hereâ€™s a breakdown of what was considered, what worked, and what didnâ€™t â€” with the rationale behind choosing Beam for the core transformation job.

### ğŸ Pure Python Approach (Baseline)

* **What we tried**: Looping over GZipped JSONL files, parsing, deduplicating, and writing to Parquet manually using PyArrow.
* **Pros**:

  * Lightweight and easy to debug.
  * Direct control over logic and edge cases.
* **Cons**:

  * ğŸ¢ **Slow** for large volumes (\~500K+ records).
  * âŒ Memory bottlenecks due to large all_events list.
  * âŒ No native parallelism or partitioned processing.
* **Conclusion**: Great for prototyping, but not scalable.

---

### ğŸ”¥ PySpark (RDD/DataFrame API)

* **What we tried**: Using Spark to load raw JSONL files and transform data using RDD and DataFrame operations.
* **Pros**:

  * ğŸ” Built-in parallelism and powerful for **huge** datasets.
  * ğŸ” Good for structured, tabular data.
* **Cons**:

  * ğŸ§± Heavy setup: required Java, Spark binaries, or PySpark setup.
  * ğŸ˜µ Overkill for our batch size (\~0.5â€“1M records).
  * ğŸš« Complex I/O when dealing with JSONL + nested fields.
* **Conclusion**: Good for production-scale big data; **too heavy** for our assignment and local run needs.

---

### ğŸ§© Apache Beam (Final Choice)

* **What worked**:

  * Out-of-the-box parallelism, batching, and grouping.
  * Easy deduplication via CombinePerKey.
  * Controlled partitioned writes per day using Beam transforms.
* **Tradeoffs**:

  * Slightly more setup than pure Python.
  * Slower than Polars in wall-time (\~15 mins for all steps).
  * Still felt â€œstructuredâ€ without being too bloated.

> ğŸ’¡ **Why Beam?**
> It struck the right balance between:

* **Scalability** for future growth.
* **Parallel-friendly** I/O and grouping logic.
* **Ease of local development** without needing a cluster.

---

## ğŸ†š Why Not Use Spark or Polars for Processing?

| Feature                            | Pure Python | PySpark       | Polars                     | Apache Beam (âœ…)    |
| ---------------------------------- | ----------- | ------------- | -------------------------- | ------------------ |
| Easy to get started                | âœ…           | âŒ             | âœ…                          | âš ï¸ (moderate)      |
| Handles JSONL                      | âš ï¸          | âœ… (w/ schema) | âš ï¸ (nested is tricky)      | âœ…                  |
| Handles dedup + grouping           | âŒ           | âœ…             | âŒ (manual logic)           | âœ…                  |
| Fast for <1M rows                  | âœ…           | âŒ (setup tax) | âœ…                          | âš ï¸ (slightly slow) |
| Works well with partitioned output | âŒ           | âœ…             | âš ï¸ (needs manual batching) | âœ…                  |

---

## ğŸ—£ï¸ Side Conversations That Influenced Design

* **Should we use a database?**
  Decided against it due to setup complexity and no real need for real-time queries. Filesystem-based pipeline was leaner.

* **Would DuckDB be better for analysis?**
  Possibly! Especially for SQL-style slicing, but Polars was already performant and fit better with our dev flow.

* **Do we need orchestration?**
  Not for the assignment. But in production, wrapping everything in main.py or using tools like **Airflow**/**Dagster**/**Argo** would help.

Great point â€” letâ€™s wrap that into the summary to show your architectural foresight. Here's the final updated section for your README-style project notes:

---

## â˜ï¸ Why Apache Beam? Especially for GCP Monitoring?

In the context of a **Google Workspace audit activity monitoring pipeline**, Apache Beam was the most **natural and strategic fit**.

### ğŸ”§ Beamâ€™s Fit for GCP:

* âœ… **Native integration with GCP services**: Beam pipelines can be executed on **Dataflow**, Googleâ€™s fully managed distributed processing service.
* âœ… **Scalable for future real-time pipelines**: Although we ran it locally, this pipeline could easily scale to handle millions of rows and stream processing using the same Beam code.
* âœ… **Designed for audit-style workloads**: The Beam model of PCollections and transforms maps very naturally to activity logs, deduplication, grouping, partitioned writes.

### ğŸ§  Final Thought:

Choosing Beam aligned with the GCP-based nature of the assignment and would allow seamless migration to production in a GCP environment if needed.

---

## âš–ï¸ Nuanced Trade-offs and Considerations

### ğŸ§© 1. **Apache Beam vs. Other ETL Frameworks**

| Tool            | Pros                                                                 | Cons                                                         | Verdict                                               |
| --------------- | -------------------------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------- |
| **Apache Beam** | Portable, scalable, GCP-friendly, supports batch + stream processing | Slightly steeper learning curve, heavier setup for local dev | âœ… Chosen: Scales to Dataflow, fits GCP audit use case |
| **Pandas**      | Fast prototyping, rich ecosystem                                     | Not memory-efficient for large-scale JSONL; single-machine   | âŒ Not suitable for heavy or long-term workloads       |
| **Polars**      | Fast, efficient, great for columnar/parquet; simple API              | Limited support for deeply nested JSON structures            | âœ… Used for analytics; âŒ Not for raw parsing           |
| **PySpark**     | Scales massively, supports JSON well                                 | Heavyweight setup, unnecessary for current scale             | âŒ Overkill for this project                           |

---

### ğŸ—‚ï¸ 2. **Partitioning Strategy**

* **Time-based partitioning by day and hour**
  âœ… Helps incremental loading and isolates corrupted data.
  âœ… Aligns with common analytics patterns (e.g., hourly dashboards).
  âš ï¸ Could lead to small files, which are inefficient for certain engines.

* **Per-run JSONL logs**
  âœ… Adds traceability for auditing and debugging.
  âœ… Great for rerunning specific batches or comparing pipelines.
  âš ï¸ May duplicate data already in partitioned files if not filtered.

---

### ğŸ“„ 3. **Data Storage Format**

| Format      | Reason Used                  | Trade-offs                                             |
| ----------- | ---------------------------- | ------------------------------------------------------ |
| .jsonl.gz | API returns JSON; streamable | âœ… Append-friendly, readable                            |
| .parquet  | For processed analytics      | âœ… Columnar, compact, fast for filters                  |
| âŒ SQL DB    | Not used                     | âš ï¸ Setup overhead, unnecessary for read-most workloads |

---

### ğŸ§® 4. **Analytics Layer: Polars vs DuckDB**

* **Polars**: Chosen for performance and simplicity in batch-oriented aggregation.

  * âœ… Efficient in-memory operations
  * âœ… Familiar API for pandas users
  * âš ï¸ Lacks seamless nested JSON handling

* **DuckDB**: Discussed as an alternative for ad-hoc SQL-based analysis.

  * âœ… Super fast for Parquet + SQL workflows
  * âœ… Would reduce memory footprint for large joins
  * âŒ Not used this time, but a great pick for next version

---

### ğŸ§  5. **Buffering Writes**

* **Buffered event writing (5K, 2.5K buffers)**:

  * âœ… Improved write performance
  * âœ… Prevented excessive I/O for every event
  * âœ… Ensured per-run file didn't bloat unmanageably
  * âš ï¸ Requires careful flush logic to avoid data loss at shutdown

---

### ğŸ§ª 6. **Testing vs Observability**

* **No orchestrator yet** but:

  * âœ… Timestamps persisted in state.json ensure fault-tolerance
  * âœ… Logging shows progress and retry status
  * âš ï¸ No real-time metrics unless integrated with tools like Stackdriver, Prometheus

---

### ğŸ”Œ 7. **Future Considerations**

| Enhancement                            | Reason to Consider                       |
| -------------------------------------- | ---------------------------------------- |
| Use of **DuckDB**                      | More flexible SQL-based exploration      |
| Add **streaming support**              | Realtime or near-realtime alerting       |
| Add **orchestration** (e.g., Airflow)  | For running fetch-process-analyze as DAG |
| Build **data catalog/schema registry** | Improve discoverability and validation   |

## ğŸ” Additional Details & Refinements to Include

### ğŸ§± **File Management / IO Optimizations**

* **Compression trade-offs**:

  * Used .jsonl.gz with gzip for raw logs.

    * âœ… Keeps disk usage small.
    * âš ï¸ gzip is slow for compression/decompression compared to zstd or snappy.
    * â—snappy preferred for Parquet (already used) â€” might note *why* you kept gzip for raw.
* **Small file problem**:

  * If you reprocess or re-fetch hourly, many small files can degrade performance downstream (e.g., S3 + Spark).
  * ğŸ“Œ *Future*: Consider compacting hourly JSONL into daily Parquet before long-term retention or analytics.

---

### ğŸ” **Retry & Fault Tolerance Enhancements**

* **Resumability edge case**:

  * If the job fails mid-run (e.g., after partial fetch + before flush), the state file may falsely advance. You could:

    * Use **"tentative timestamp advancement"** â€” save state only *after* successful flush.
    * Consider adding a â€œcheckpointed flushâ€ per buffer.
* **Overlapping logic**:

  * 3-minute overlap is a great default, but:

    * ğŸ“Œ Consider making it **dynamically adjustable** based on observed skew in event arrival times (e.g., percentile delay).

---

### ğŸ§  **Model Validation & Schema Drift**

* **Pydantic for schema evolution**:

  * âœ… Already used to validate raw events.
  * You might log or capture fields *not* in the current model (i.e., unexpected keys in events).

    * This helps you detect schema drift silently.
    * Add optional "extra_fields" field to hold unmapped keys or log unknown keys in a side channel.
* **Schema registry (future)**:

  * Versioned Pydantic models with @version tags â€” this can help compare model changes over time for future-proofing.

---

### ğŸ”§ **Beam Pipeline Enhancements**

* **WritePath customization**:

  * You write daily Parquet files. If you switch to per-hour partitioning (YYYY-MM-DD-HH.parquet) it gives better slicing granularity.
* **Sorting by timestamp before Parquet write**:

  * Not strictly required, but improves query performance on timestamp-based filters.
  * You did add this â€” great! Just mention it **explicitly** under transformation performance.

---

### ğŸ“Š **Analytics Ideas (Polars Layer)**

* Consider adding:

  * **Per-user activity patterns** over time (e.g., login bursts).
  * **Volume anomaly detection** (bytes sent, requests/hour).
  * **Most frequent method_name per hour/day/user**.
* ğŸ“ˆ Could pre-define and cache these as **Polars scripts** or Parquet summary files for dashboarding.

---

### âš ï¸ **Duplication Risk Handling**

* **Unique ID + Overlap logic**:

  * You dedup using unique_id â€” which is perfect.
  * Consider asserting **strict ordering** (or logging violations) during transformation â€” if timestamps are ever back in time.

---

### ğŸ” **Security / Sensitivity Considerations**

* **PII/Email handling**:

  * actor.email is personal info â€” if storing logs long-term, consider hashing, obfuscating, or protecting with IAM rules.
* âœ… You kept logs local â€” but good to mention this is **non-production** assumption.

---

### ğŸš¦ **Benchmarking & Performance Metrics**

* If you're performance-conscious, consider:

  * **Fetch throughput (events/sec)**.
  * **Processing rate in Beam (rows/sec)**.
  * **Time to Parquet write (time/file or MB/s)**.
* Even basic timing logs (you have these!) + a metrics.md log can serve as a future benchmark baseline.

---

### ğŸŒ **Cross-cutting Ideas**

| Area                      | Suggestion                                                                           |
| ------------------------- | ------------------------------------------------------------------------------------ |
| **Partition overwrite**   | Consider writing to temp location â†’ move to final â†’ prevents corruption              |
| **Catalog integration**   | Write .schema.json alongside Parquet â†’ self-describing datasets                    |
| **Command orchestration** | Wrap fetch + transform + analyze into a main.py CLI via typer or fire          |
| **Async fetch**           | Not urgent, but could parallelize pages if response token can be split               |
| **Output manifest**       | Store a manifest.json for each run: what files were written, when, how many events |

---

## ğŸ§¾ Optional Log Entries to Consider Adding

You already have great logging. Here are minor refinements:

| Log Additions                         | Purpose                                        |
| ------------------------------------- | ---------------------------------------------- |
| Event counts per hour (before write)  | Visibility into hourly density                 |
| Retry count per request (if retried)  | Helps in diagnosing throttling/API issues      |
| Overlap effectiveness check           | Log if any overlapping event was a duplicate   |
| Partition size stats (before Parquet) | Log how many events written per day file       |
| Gzip write time                       | Useful since this was a performance bottleneck |

---

## ğŸš€ Future-Proofing for Production

* **Move to Dataflow**:

  * Youâ€™re Beam-ready â†’ just change the runner and push the code.
* **Auto-scaling and streaming**:

  * Pipeline logic supports streaming (if API + Source allow).
* **CI/CD**:

  * Add basic GitHub Actions for pytest, lint, and scheduled runs.
* **Metadata manifest**:

  * Store last run, number of events fetched, time taken, output path etc., in a structured metadata file (run_<ts>.json).

---

## âœ… TL;DR: Top Additions Worth Including

1. âœ… Compression tradeoff (gzip vs snappy/zstd).
2. âœ… Future: schema evolution, unknown field handling.
3. âœ… Sorting before Parquet write â€” log explicitly.
4. âœ… Partition-level metrics (rows, duplicates, write time).
5. âœ… Logs for retries, dedup counts, overlap efficiency.
6. âœ… Manifest or metadata registry per run.
7. âœ… Polars analysis ideas: anomaly detection, aggregation.
8. âœ… Logging violations in ordering if ever happens.
9. âœ… Optionally: metrics.md or benchmark log for future speedups.
10. âœ… Add main.py CLI orchestration entrypoint.

---

## ğŸ§© Additional Enhancements You Could Mention

### 1. ğŸ”„ **Data Lineage / Provenance**

* **Why?** For audit-focused systems (like token activity), traceability is key.
* **Suggestion:** Add metadata to each Parquet file with:

  * Source file(s)
  * Record count
  * Timestamps (min, max)
  * Processing timestamp
* Could be stored as .meta.json alongside each .parquet.

---

### 2. ğŸ›‚ **Permissioning + Key Management**

* You already use a service account, which is great.
* Mention that in production:

  * Credentials should be rotated and stored via **secret managers** (e.g., GCP Secret Manager).
  * **IAM roles** should restrict the scope to only needed audit APIs.

---

### 3. ğŸ§ª **Validation / Backfill Strategy**

* What if the pipeline goes down or state is corrupted?
* Consider adding:

  * **Backfill mode** with a start/end timestamp override.
  * **Sanity checks** like:

    * Event time not in future
    * Partition path matches event time
    * No duplicated event IDs in partition

---

### 4. ğŸ› ï¸ **Schema Evolution Awareness**

* You already use Pydantic for validation â€” excellent.
* To go further:

  * Store a **model version** in each record.
  * Maintain versioned schemas (e.g., schema_v1.json, v2/RawTokenActivity) to detect drift.
  * Add an **alert if new fields appear** or expected fields go missing.

---

### 5. ğŸ“‰ **Compression + Storage Strategy**

* GZIP is okay, but:

  * Consider comparing **zstandard (zstd)** or **brotli** for better compression speed & ratio.
  * You can document **benchmark results** in compression_eval.md.

---

### 6. ğŸš¦ **Fail-safe Execution**

* Currently fetcher.py and processor.py assume everything will complete.
* Add resilience by:

  * Logging **partial failure**
  * Using a **run manifest** (status: success/failed, error messages)
  * **Not updating state** unless all steps succeed

---

### 7. ğŸ“Š **Monitoring Dashboard (Future)**

If this were a prod system:

* You could pipe log events (counts, durations, retries) to **Prometheus / Grafana** or **Stackdriver**.
* Helps understand:

  * Is the API getting slower?
  * Are more events fetched per run?
  * Is processing lag increasing?

---

### 8. ğŸ”„ **Replay Mode**

* In a prod system:

  * You may want to **replay a single hour or day** (e.g., if corrupt or missing).
* Add CLI arg to process a specific partition:
  python main.py --replay 2024-05-26 --hour 13

---

## ğŸ“Œ Bonus: Interview/Presentation Tips

If you're asked about your design in an interview:

* âœ… Emphasize *"decisions were made based on current scale and assignment scope."*
* âœ… Clarify that youâ€™ve thought about **observability, future scaling, and fault tolerance**.
* âœ… Show that the pipeline is **modular** and **ready to grow** into a real-world deployment.